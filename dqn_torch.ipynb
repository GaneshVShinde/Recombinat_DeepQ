{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils_data\n",
    "import sklearn.datasets as datasets\n",
    "import network\n",
    "\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state','action','next_state','reward')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self,capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position=0\n",
    "    \n",
    "    def push(self,*args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position +1)%self.capacity\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self,layers):\n",
    "        self.policy_net=network.Network(layers) # policy network\n",
    "        self.memory = ReplayMemory(1000)\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        self.state_dimention = layers[0]\n",
    "        self.n_action = layers[-1]\n",
    "        self.nlayers = layers\n",
    "        self.gamma = 0.95\n",
    "        \n",
    "    def select_action(self,state,eps=0.01):\n",
    "        sample=random.random()\n",
    "        state\n",
    "        if sample<eps:\n",
    "            return random.choice(range(self.n_action))\n",
    "        else:\n",
    "            return torch.max(self.policy_net(state),0)[1].tolist()\n",
    "    \n",
    "    def learn_model(self):\n",
    "        if len(self.memory)<BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.tensor(batch.state)\n",
    "        nextState_batch = torch.tensor(batch.next_state)\n",
    "        reward_batch = torch.tensor(batch.reward)\n",
    "        action_batch = torch.tensor(batch.action)\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch.float())\n",
    "        \n",
    "        next_state_value = self.policy_net(nextState_batch.float()).max(1)[0].detach()\n",
    "        \n",
    "        expected_rewards=next_state_value*self.gamma+reward_batch.float()\n",
    "        \n",
    "        target_action_values= state_action_values.clone()\n",
    "        \n",
    "        r_idx=torch.arange(target_action_values.size(0)).long()\n",
    "        \n",
    "        target_action_values[r_idx,action_batch] = expected_rewards\n",
    "        \n",
    "        loss = F.smooth_l1_loss(state_action_values,target_action_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        for param in self.policy_net.parameters():#Gradient cliping to \n",
    "            param.grad.data.clamp_(-1,1)\n",
    "            \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def remember(self,state,action,next_state,reward):\n",
    "        self.memory.push(state,action,next_state,reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "    def __init__(self,layers):\n",
    "        self.policy_net=network.Network(layers) # policy network\n",
    "        self.target_net=network.Network(layers) # target network \n",
    "        self.memory = ReplayMemory(1000)\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        self.state_dimention = layers[0]\n",
    "        self.n_action = layers[-1]\n",
    "        self.nlayers = layers\n",
    "        self.gamma = 0.95\n",
    "        \n",
    "    \n",
    "    def copy_policy_net_To_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "    \n",
    "    def select_action(self,state,eps=0.01):\n",
    "        sample=random.random()\n",
    "        \n",
    "        if sample<eps:\n",
    "            return random.choice(range(self.n_action))\n",
    "        else:\n",
    "            return torch.max(self.policy_net(state),0)[1].tolist()\n",
    "    \n",
    "    def learn_model(self):\n",
    "        if len(self.memory)<BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "\n",
    "        state_batch = torch.tensor(batch.state)\n",
    "        nextState_batch = torch.tensor(batch.next_state)\n",
    "        reward_batch = torch.tensor(batch.reward)\n",
    "        action_batch = torch.tensor(batch.action)\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch.float())\n",
    "        \n",
    "        next_state_value = self.target_net(nextState_batch.float()).max(1)[0].detach()\n",
    "        \n",
    "        expected_rewards=next_state_value*GAMMA+reward_batch.float()\n",
    "        \n",
    "        target_action_values= state_action_values.clone()\n",
    "        \n",
    "        r_idx=torch.arange(target_action_values.size(0)).long()\n",
    "        \n",
    "        target_action_values[r_idx,action_batch] = expected_rewards\n",
    "        \n",
    "        loss = F.smooth_l1_loss(state_action_values,target_action_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1,1)\n",
    "            \n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def remember(self,state,action,next_state,reward):\n",
    "        self.memory.push(state,action,next_state,reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "env = gym.make('CartPole-v0').env\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent=DDQN([state_size,10,10,action_size])\n",
    "done = False\n",
    "BATCH_SIZE = 128\n",
    "argv='monitor'\n",
    "filename=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "done=False\n",
    "t=0\n",
    "num_episodes = 100000\n",
    "if 'monitor' in argv:\n",
    "    #filename = os.path.basename(__file__).split('.')[0]\n",
    "    monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "    env = wrappers.Monitor(env, monitor_dir)\n",
    "for e in range(num_episodes):\n",
    "    state =env.reset()\n",
    "    t=0\n",
    "    while not done and t<10000:\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END)* math.exp(-1. * t / EPS_DECAY)\n",
    "        action = agent.select_action(torch.from_numpy(state).float())\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "        reward = reward if not done else -10\n",
    "        \n",
    "        agent.remember(state.tolist(),action,next_state.tolist(),reward)\n",
    "        state = next_state\n",
    "        agent.learn_model()\n",
    "        print(t)\n",
    "        if done and t<199:\n",
    "            reward =-300\n",
    "        t += 1\n",
    "    \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(\"test\").split('.')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.899575106232294\n",
      "0.8991504248583688\n",
      "0.8987259557720543\n",
      "0.8983016988672331\n",
      "0.8978776540378411\n",
      "0.8974538211778671\n",
      "0.8970302001813527\n",
      "0.8966067909423928\n",
      "0.896183593355135\n",
      "0.89576060731378\n",
      "0.8953378327125813\n",
      "0.894915269445845\n",
      "0.8944929174079306\n",
      "0.8940707764932498\n",
      "0.8936488465962676\n",
      "0.8932271276115016\n",
      "0.8928056194335218\n",
      "0.8923843219569512\n",
      "0.8919632350764655\n",
      "0.8915423586867929\n",
      "0.8911216926827145\n",
      "0.8907012369590634\n",
      "0.8902809914107261\n",
      "0.8898609559326409\n",
      "0.8894411304197992\n",
      "0.8890215147672444\n",
      "0.8886021088700727\n",
      "0.8881829126234326\n",
      "0.8877639259225251\n",
      "0.8873451486626033\n",
      "0.886926580738973\n",
      "0.8865082220469924\n",
      "0.8860900724820715\n",
      "0.8856721319396732\n",
      "0.8852544003153122\n",
      "0.8848368775045556\n",
      "0.8844195634030227\n",
      "0.8840024579063851\n",
      "0.8835855609103663\n",
      "0.883168872310742\n",
      "0.8827523920033402\n",
      "0.8823361198840407\n",
      "0.8819200558487755\n",
      "0.8815041997935286\n",
      "0.881088551614336\n",
      "0.8806731112072855\n",
      "0.8802578784685173\n",
      "0.879842853294223\n",
      "0.8794280355806463\n",
      "0.8790134252240828\n",
      "0.8785990221208799\n",
      "0.878184826167437\n",
      "0.8777708372602048\n",
      "0.8773570552956863\n",
      "0.8769434801704359\n",
      "0.8765301117810599\n",
      "0.8761169500242162\n",
      "0.8757039947966141\n",
      "0.8752912459950152\n",
      "0.874878703516232\n",
      "0.8744663672571289\n",
      "0.874054237114622\n",
      "0.8736423129856787\n",
      "0.873230594767318\n",
      "0.8728190823566101\n",
      "0.8724077756506773\n",
      "0.8719966745466925\n",
      "0.8715857789418806\n",
      "0.8711750887335178\n",
      "0.8707646038189315\n",
      "0.8703543240955004\n",
      "0.8699442494606546\n",
      "0.8695343798118754\n",
      "0.8691247150466955\n",
      "0.8687152550626985\n",
      "0.8683059997575197\n",
      "0.867896949028845\n",
      "0.8674881027744118\n",
      "0.8670794608920085\n",
      "0.8666710232794748\n",
      "0.8662627898347011\n",
      "0.865854760455629\n",
      "0.8654469350402515\n",
      "0.8650393134866119\n",
      "0.864631895692805\n",
      "0.8642246815569761\n",
      "0.863817670977322\n",
      "0.8634108638520896\n",
      "0.8630042600795776\n",
      "0.862597859558135\n",
      "0.8621916621861613\n",
      "0.8617856678621075\n",
      "0.8613798764844748\n",
      "0.8609742879518154\n",
      "0.8605689021627324\n",
      "0.8601637190158791\n",
      "0.8597587384099596\n",
      "0.8593539602437291\n",
      "0.8589493844159929\n"
     ]
    }
   ],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2000\n",
    "import math\n",
    "for t  in range(100):\n",
    "    print(EPS_END + (EPS_START - EPS_END)* math.exp(-1. * t / EPS_DECAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.037493615117251355,\n",
       " -0.029122303595574574,\n",
       " 0.012974990787389215,\n",
       " -0.033191755329343015]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=env.reset()\n",
    "s.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=net(state_batch.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02547568, -0.18396779, -0.0259671 ,  0.2724869 ]), 1.0, False, {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=net2(state_batch.float()).max(1)[0].detach()+reward_batch.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cln=tt.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 2, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3086, -0.4095, -0.2798],\n",
       "        [ 0.2973, -0.2378, -0.4622],\n",
       "        [ 0.3305, -0.1605, -0.5672],\n",
       "        [ 0.2659, -0.3189, -0.3751],\n",
       "        [ 0.3075, -0.2131, -0.4892],\n",
       "        [ 0.3332, -0.4646, -0.2286],\n",
       "        [ 0.2872, -0.2626, -0.4353]], grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cln[:,action_batch] = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2281,  4.3208,  8.4637,  2.2620,  5.3565, -0.7846,  3.2904])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem=ReplayMemory(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mem.push([1,2,i],random.choice(range(3)),i+1,i-1)  for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Student(state=[1, 2, 9], action=2, next_state=10, reward=8),\n",
       " Student(state=[1, 2, 0], action=1, next_state=1, reward=-1),\n",
       " Student(state=[1, 2, 1], action=2, next_state=2, reward=0),\n",
       " Student(state=[1, 2, 2], action=0, next_state=3, reward=1),\n",
       " Student(state=[1, 2, 3], action=1, next_state=4, reward=2),\n",
       " Student(state=[1, 2, 4], action=0, next_state=5, reward=3),\n",
       " Student(state=[1, 2, 5], action=0, next_state=6, reward=4),\n",
       " Student(state=[1, 2, 6], action=0, next_state=7, reward=5),\n",
       " Student(state=[1, 2, 7], action=2, next_state=8, reward=6),\n",
       " Student(state=[1, 2, 8], action=1, next_state=9, reward=7)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=mem.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Transition(*zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=torch.arange(cln.size(0)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cln[j,action_batch] = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3086, -0.4095,  0.2281],\n",
       "        [ 4.3208, -0.2378, -0.4622],\n",
       "        [ 0.3305, -0.1605,  8.4637],\n",
       "        [ 0.2659,  2.2620, -0.3751],\n",
       "        [ 5.3565, -0.2131, -0.4892],\n",
       "        [ 0.3332, -0.7846, -0.2286],\n",
       "        [ 3.2904, -0.2626, -0.4353]], grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
